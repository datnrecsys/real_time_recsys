{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dill \n",
    "from pydantic import BaseModel\n",
    "import sys\n",
    "import os\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from loguru import logger\n",
    "from load_dotenv import load_dotenv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "import mlflow\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from src.utils.embedding_id_mapper import IDMapper\n",
    "from src.algo.sequence_ranker.model import SequenceRatingPrediction\n",
    "from src.algo.sequence_ranker.dataset import UserItemBinaryRatingDFDataset\n",
    "from src.algo.sequence_ranker.trainer import SeqModellingLitModule\n",
    "from src.algo.item2vec.trainer import LitSkipGram\n",
    "from src.algo.item2vec.model import SkipGram\n",
    "from src.eval.utils import create_rec_df, create_label_df, merge_recs_with_target\n",
    "from src.eval.log_metrics import log_ranking_metrics, log_classification_metrics\n",
    "from src.utils.data_prep import chunk_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(BaseModel):\n",
    "    testing: bool = False\n",
    "    log_to_mlflow: bool = True\n",
    "    experiment_name: str = \"pruning-sequence-modelling\"\n",
    "    notebook_persit_dp: str = None\n",
    "    \n",
    "    run_name: str = None\n",
    "\n",
    "    user_col: str = \"user_id\"\n",
    "    item_col: str = \"parent_asin\"\n",
    "    rating_col: str = \"rating\"\n",
    "    timestamp_col: str = \"timestamp\"\n",
    "    group_name: str = \"seq-modelling\"\n",
    "    item_metadata_pipeline_fp: str = \"../data_for_ai/interim/item_metadata_pipeline_wo_user_item_manipulate.dill\"\n",
    "    tfm_chunk_size: int = 1000\n",
    "\n",
    "    top_K: int = 100\n",
    "    top_k: int = 10\n",
    "\n",
    "    batch_size: int = 512\n",
    "    learning_rate: float = 0.001\n",
    "    l2_reg: float = 1e-6\n",
    "    early_stopping_patience: int = 5\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    max_epochs: int = 100\n",
    "\n",
    "    # TwoTower specific\n",
    "    dropout: float = 0.3\n",
    "    embedding_dim: int = 256\n",
    "    use_user_embedding: bool = True\n",
    "    user_embedding_dim: int = 32\n",
    "\n",
    "    use_title: bool = True\n",
    "    title_embedding_dim: int = 384\n",
    "    title_fc_dim: int = 32\n",
    "\n",
    "    use_extra_item_embedding: bool = True\n",
    "\n",
    "    train_data_fp: str = os.path.abspath(\"../data_for_ai/interim/train_sample_interactions_16407u_features_neg_seq_without_stats_item_user.parquet\")\n",
    "    val_data_fp: str = os.path.abspath(\"../data_for_ai/interim/val_sample_interactions_16407u_features_neg_seq_without_stats_item_user.parquet\")\n",
    "\n",
    "    best_checkpoint_path: str = None\n",
    "    def init(self):\n",
    "        self.run_name: str = f\"(3)(8)(17) tune params encoder transformer head=8\"\n",
    "        self.notebook_persit_dp = os.path.abspath(f\"data/{self.experiment_name}/{self.run_name}\")\n",
    "\n",
    "        if not (mlflow_uri := os.environ.get(\"MLFLOW_TRACKING_URI\")):\n",
    "            self.log_to_mlflow = False\n",
    "            logger.warning(\"MLFlow is not enabled. Turn off tracking to Mlflow.\")\n",
    "\n",
    "        if self.log_to_mlflow:\n",
    "            logger.info(\n",
    "                f\"Setting up Mlflow experiment: {self.experiment_name}, run_name: {self.run_name}\"\n",
    "            )\n",
    "\n",
    "            self._mlf_logger = MLFlowLogger(\n",
    "                experiment_name=self.experiment_name,\n",
    "                run_name=self.run_name,\n",
    "                tracking_uri=mlflow_uri,\n",
    "                log_model=False,\n",
    "            )\n",
    "\n",
    "        if not self.testing:\n",
    "            os.makedirs(self.notebook_persit_dp, exist_ok=True)\n",
    "        return self\n",
    "    \n",
    "args = Args().init()\n",
    "print(args.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.item_metadata_pipeline_fp, \"rb\") as f:\n",
    "    item_metadata_pipeline = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metadata_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load title embedding\n",
    "title_embedding_matrix = np.load(\"../data_for_ai/interim/metadata_title_embedding.npy\")  # shape (4817, 898)\n",
    "# add 2 more vector for padding and start token \n",
    "title_embedding_matrix = np.concatenate(\n",
    "    [np.zeros((2, title_embedding_matrix.shape[1])), title_embedding_matrix], axis=0\n",
    ")\n",
    "title_embedding_tensor = torch.tensor(title_embedding_matrix, dtype=torch.float32)\n",
    "title_embedding_layer = nn.Embedding.from_pretrained(\n",
    "    embeddings=title_embedding_tensor,\n",
    "    freeze=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Title embedding shape: {title_embedding_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(n_users, n_items, embedding_dim, dropout, item_embedding=None,\n",
    "               user_embedding_dim=None, use_user_embedding=False,\n",
    "               use_title=False, title_embedding=None,\n",
    "               title_embedding_dim=898, title_fc_dim=256,\n",
    "               use_extra_item_embedding=False, extra_item_embedding=None, extra_item_embedding_dim=None):\n",
    "    return SequenceRatingPrediction(\n",
    "        item_embedding=item_embedding,\n",
    "        num_users=n_users,\n",
    "        num_items=n_items,\n",
    "        embedding_dim=embedding_dim,\n",
    "        dropout=dropout,\n",
    "        user_embedding_dim=user_embedding_dim,\n",
    "        use_user_embedding=use_user_embedding,\n",
    "        use_title=use_title,\n",
    "        title_embedding=title_embedding,\n",
    "        title_embedding_dim=title_embedding_dim,\n",
    "        title_fc_dim=title_fc_dim,\n",
    "        use_extra_item_embedding=use_extra_item_embedding,\n",
    "        extra_item_embedding_dim=extra_item_embedding_dim,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "user_embedding_dim = 64\n",
    "batch_size = 2\n",
    "\n",
    "# Mock data\n",
    "user_indices = [0, 0, 1, 2, 2]\n",
    "item_indices = [0, 1, 2, 3, 4]\n",
    "timestamps = [0, 1, 2, 3, 4]\n",
    "ratings = [0, 3, 1, 3, 0]\n",
    "# item_sequences = [\n",
    "#     [2, 3, -1, -1],\n",
    "#     [2, 4, -1, -1],\n",
    "#     [1, 3, -1, -1],\n",
    "#     [2, 1, -1, -1],\n",
    "#     [4, 1, -1, -1],\n",
    "# ]\n",
    "\n",
    "item_sequences = [\n",
    "    [-1, -1, 2, 3],\n",
    "    [-1, -1, 2, 4],\n",
    "    [-1, -1, 1, 3],\n",
    "    [-1, -1, 2, 1],\n",
    "    [-1, -1, 4, 1],\n",
    "]\n",
    "\n",
    "\n",
    "n_users = len(set(user_indices))\n",
    "\n",
    "n_items = len(set(item_indices))\n",
    "\n",
    "train_df = pd.DataFrame(\n",
    "    {\n",
    "        \"user_indice\": user_indices,\n",
    "        \"item_indice\": item_indices,\n",
    "        args.timestamp_col: timestamps,\n",
    "        args.rating_col: ratings,\n",
    "        \"item_sequence\": item_sequences,\n",
    "    }\n",
    ")\n",
    "# Mock title embedding (giả sử mỗi item có 32 chiều title embedding)\n",
    "mock_title_embedding_matrix = np.random.randn(n_items, embedding_dim).astype(np.float32)\n",
    "mock_title_embedding_layer = nn.Embedding.from_pretrained(\n",
    "    embeddings=torch.tensor(mock_title_embedding_matrix),\n",
    "    freeze=True)\n",
    "\n",
    "model = init_model(n_users, \n",
    "                   n_items, \n",
    "                   embedding_dim, \n",
    "                   args.dropout,\n",
    "                   user_embedding_dim=user_embedding_dim,\n",
    "                   use_user_embedding=args.use_user_embedding,\n",
    "                   use_title=args.use_title,\n",
    "                   title_embedding=mock_title_embedding_layer,\n",
    "                   title_embedding_dim=embedding_dim,\n",
    "                   title_fc_dim=16,)\n",
    "\n",
    "# Example forward pass\n",
    "model.eval()\n",
    "user = torch.tensor([0])\n",
    "item_sequence = torch.tensor([[-0, 1, -1, -1]])\n",
    "target_item = torch.tensor([2])\n",
    "target_title = torch.tensor([2])  # giả sử title index trùng với item index\n",
    "predictions = model(user, item_sequence, target_item, target_title)\n",
    "print(predictions)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_dataset = UserItemBinaryRatingDFDataset(\n",
    "    train_df, \"user_indice\", \"item_indice\", args.rating_col, args.timestamp_col,\"item_sequence\",\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    rating_dataset, batch_size=batch_size, shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_input in train_loader:\n",
    "    print(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "lit_model = SeqModellingLitModule(model, log_dir=args.notebook_persit_dp)\n",
    "\n",
    "# train model\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=f\"{args.notebook_persit_dp}/test\",\n",
    "    max_epochs=100,\n",
    "    accelerator=args.device if args.device else \"auto\",\n",
    ")\n",
    "trainer.fit(\n",
    "    model=lit_model, train_dataloaders=train_loader, val_dataloaders=train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "user = torch.tensor([0])\n",
    "item_sequence = torch.tensor([[-1, -1, -1, 0, 1]])\n",
    "target_item = torch.tensor([2])\n",
    "target_title = torch.tensor([2])  # giả sử title index trùng với item index\n",
    "predictions = model.predict(user, item_sequence, target_item, target_title)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 1% of train data for debugging\n",
    "# For debugging, we can take a small sample of the training data\n",
    "train_df = pd.read_parquet(args.train_data_fp)\n",
    "val_df = pd.read_parquet(args.val_data_fp)\n",
    "\n",
    "assert set(val_df[args.user_col].unique()).issubset(set(train_df[args.user_col].unique())), \"Validation users must be present in training users.\"\n",
    "\n",
    "assert set(val_df[args.item_col].unique()).issubset(set(train_df[args.item_col].unique())), \"Validation items must be present in training items.\"\n",
    "assert train_df[args.timestamp_col].max() < val_df[args.timestamp_col].min(), \"Validation data must be after training data. Otherwise, its a data contamination problem.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_indices = train_df[\"user_indice\"].unique()\n",
    "item_indices = train_df[\"item_indice\"].unique()\n",
    "\n",
    "train_item_features = chunk_transform(\n",
    "    train_df, item_metadata_pipeline, chunk_size=args.tfm_chunk_size\n",
    ")\n",
    "train_item_features = train_item_features.astype(np.float32)\n",
    "\n",
    "val_item_features = chunk_transform(\n",
    "    val_df, item_metadata_pipeline, chunk_size=args.tfm_chunk_size\n",
    ")\n",
    "val_item_features = val_item_features.astype(np.float32)\n",
    "\n",
    "logger.info(f\"{len(user_indices)=:,.0f}, {len(item_indices)=:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(train_df.loc[train_df[\"user_id\"] == \"AEEV5YWQKPBTLFWHKOBBULYA2RDQ\"].sort_values(by=args.timestamp_col, ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert user_id and item_id into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idm_path = os.path.abspath(\"../data_for_ai/interim/idm_16407u.json\")\n",
    "# idm = IDMapper().load(idm_path)\n",
    "# idm.get_user_id(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.pipe(idm.map_indices)\n",
    "# val_df = val_df.pipe(idm.map_indices)\n",
    "\n",
    "# assert idm.unknown_item_index not in train_df[\"item_indice\"].values, \"Unknown item index must be present in training data.\"\n",
    "# assert idm.unknown_user_index not in train_df[\"user_indice\"].values, \"Unknown user index must be present in training data.\"\n",
    "# assert idm.unknown_item_index not in val_df[\"item_indice\"].values, \"Unknown item index must be present in validation data.\"\n",
    "# assert idm.unknown_user_index not in val_df[\"user_indice\"].values, \"Unknown user index must be present in validation data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert train_df.groupby(args.user_col)[args.item_col].nunique().min() >= 5, \"Each user must have at least five items.\"\n",
    "# assert train_df.groupby(args.item_col)[args.user_col].nunique().min() >= 10, \"Each item must have at least ten users.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_dataset = UserItemBinaryRatingDFDataset(\n",
    "    train_df, \"user_indice\", \"item_indice\", args.rating_col, args.timestamp_col, \"item_sequence\",item_feature=train_item_features,\n",
    ")\n",
    "val_rating_dataset = UserItemBinaryRatingDFDataset(\n",
    "    val_df, \"user_indice\", \"item_indice\", args.rating_col, args.timestamp_col, \"item_sequence\",item_feature=val_item_features,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    rating_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_rating_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False\n",
    ")\n",
    "\n",
    "assert isinstance(train_loader.dataset, UserItemBinaryRatingDFDataset), \"train_loader must use UserItemBinaryRatingDFDataset\"\n",
    "assert isinstance(val_loader.dataset, UserItemBinaryRatingDFDataset), \"val_loader must use UserItemBinaryRatingDFDataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the weight from SkipGram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_indices = train_df[args.item_col].unique()\n",
    "user_indices = train_df[args.user_col].unique()\n",
    "n_items = len(item_indices)\n",
    "n_users = len(user_indices)\n",
    "\n",
    "logger.info(f\"Number of users: {n_users}, Number of items: {n_items}\")\n",
    "\n",
    "assert args.embedding_dim == 256, \"Embedding dimension must be 256\"\n",
    "best_trainer = LitSkipGram.load_from_checkpoint(\n",
    "    \"../data_for_ai/interim/best-item2vec-weight.ckpt\",\n",
    "    skipgram_model=SkipGram(n_items, args.embedding_dim).to(args.device),\n",
    ")\n",
    "skipgram_item_embedding = best_trainer.skipgram_model.embeddings.weight.data.cpu()\n",
    "print(f\"SkipGram Item embedding shape: {skipgram_item_embedding.shape}\")\n",
    "print(f\"SkipGram Item embedding dtype: {skipgram_item_embedding.dtype}\")\n",
    "# convert skipgram_item_embedding into torch.nn.Embedding and take the first n_items rows\n",
    "skipgram_item_embedding = torch.nn.Embedding.from_pretrained(\n",
    "    skipgram_item_embedding[:n_items], freeze=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idm_path = os.path.abspath(\"../data_for_ai/interim/idm_16407u.json\")\n",
    "idm = IDMapper().load(idm_path)\n",
    "idm.get_user_id(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, mode=\"min\", verbose=False\n",
    ")\n",
    "\n",
    "model = init_model(n_users, n_items, args.embedding_dim, args.dropout,item_embedding=skipgram_item_embedding,\n",
    "                   user_embedding_dim=args.user_embedding_dim,\n",
    "                   use_user_embedding=args.use_user_embedding,\n",
    "                   use_title=args.use_title,\n",
    "                   title_embedding=title_embedding_layer,\n",
    "                   title_embedding_dim=args.title_embedding_dim,\n",
    "                   title_fc_dim=args.title_fc_dim,\n",
    "                   use_extra_item_embedding=args.use_extra_item_embedding,)\n",
    "\n",
    "lit_model = SeqModellingLitModule(\n",
    "    model,\n",
    "    learning_rate=args.learning_rate,\n",
    "    l2_reg=args.l2_reg,\n",
    "    log_dir=args.notebook_persit_dp,\n",
    "    accelerator=args.device,\n",
    "    idm= idm\n",
    ")\n",
    "\n",
    "log_dir = f\"{args.notebook_persit_dp}/logs/overfit\"\n",
    "\n",
    "# train model\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=log_dir,\n",
    "    accelerator=args.device if args.device else \"auto\",\n",
    "    max_epochs=100,\n",
    "    overfit_batches=1,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "# trainer.fit(\n",
    "#     model=lit_model,\n",
    "#     train_dataloaders=train_loader,\n",
    "#     val_dataloaders=train_loader,\n",
    "# )\n",
    "# logger.info(f\"Logs available at {trainer.log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning\n",
    "print(lightning.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_roc_auc\", patience=args.early_stopping_patience, mode=\"max\", verbose=False, min_delta=0.001\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"{args.notebook_persit_dp}/checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_roc_auc\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "model = init_model(n_users, n_items, \n",
    "                   args.embedding_dim, \n",
    "                   args.dropout, \n",
    "                   item_embedding=skipgram_item_embedding,\n",
    "                   user_embedding_dim=args.user_embedding_dim,\n",
    "                   use_user_embedding=args.use_user_embedding,\n",
    "                   use_title=args.use_title,\n",
    "                   title_embedding=title_embedding_layer,\n",
    "                   title_embedding_dim=args.title_embedding_dim,\n",
    "                   title_fc_dim=args.title_fc_dim,\n",
    "                   use_extra_item_embedding=True,\n",
    "                   extra_item_embedding_dim=train_item_features.shape[1],)\n",
    "\n",
    "print(f\"Model: {model}\")\n",
    "lit_model = SeqModellingLitModule(\n",
    "    model,\n",
    "    learning_rate=args.learning_rate,\n",
    "    l2_reg=args.l2_reg,\n",
    "    log_dir=args.notebook_persit_dp,\n",
    "    accelerator=args.device,\n",
    "    idm= idm\n",
    ")\n",
    "\n",
    "log_dir = f\"{args.notebook_persit_dp}/logs/run\"\n",
    "\n",
    "# train model\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=log_dir,\n",
    "    accelerator=args.device if args.device else \"auto\",\n",
    "    max_epochs=args.max_epochs,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    logger=args._mlf_logger if args.log_to_mlflow else None,\n",
    "    # limit_train_batches=0.1    \n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model=lit_model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    "    \n",
    ")\n",
    "\n",
    "# Change the library as a workaround for the issue in the latest Lightning release\n",
    "#https://github.com/Lightning-AI/pytorch-lightning/pull/20669/commits/429f732a0528c558e701da7ec01e51c1e2e4f32e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Loading best checkpoint from {checkpoint_callback.best_model_path}...\")\n",
    "args.best_checkpoint_path = checkpoint_callback.best_model_path\n",
    "\n",
    "best_trainer = SeqModellingLitModule.load_from_checkpoint(\n",
    "    checkpoint_path=checkpoint_callback.best_model_path,\n",
    "    model=init_model(n_users, n_items, \n",
    "                   args.embedding_dim, \n",
    "                   args.dropout, \n",
    "                   item_embedding=skipgram_item_embedding,\n",
    "                   user_embedding_dim=args.user_embedding_dim,\n",
    "                   use_user_embedding=args.use_user_embedding,\n",
    "                   use_title=args.use_title,\n",
    "                   title_embedding=title_embedding_layer,\n",
    "                   title_embedding_dim=args.title_embedding_dim,\n",
    "                   title_fc_dim=args.title_fc_dim,\n",
    "                   use_extra_item_embedding=True,\n",
    "                   extra_item_embedding_dim=train_item_features.shape[1],),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = best_trainer.model.to(args.device)\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_recs_df = val_df.sort_values(by=args.timestamp_col).drop_duplicates(subset=[args.user_col], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.start_run(run_id = trainer.logger.run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_user_indices = val_df[\"user_indice\"].values\n",
    "val_item_indices = val_df[\"item_indice\"].values\n",
    "val_item_sequences = val_df[\"item_sequence\"].values.tolist()\n",
    "# take item features for the items in val_features\n",
    "val_item_features = val_item_features[val_item_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = torch.tensor(val_user_indices, device=args.device)\n",
    "item_sequences = torch.tensor(val_item_sequences, device=args.device)\n",
    "items = torch.tensor(val_item_indices, device=args.device)\n",
    "item_embeddings = torch.tensor(val_item_features, device=args.device)\n",
    "classifications = best_model.predict(users, item_sequences, items, item_embeddings=item_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_classification_df = val_df.assign(\n",
    "    classification_proba=classifications.cpu().detach().numpy(),\n",
    "    label=lambda df: df[args.rating_col].gt(0).astype(int),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_classification_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report = log_classification_metrics(\n",
    "    args,\n",
    "    eval_classification_df,\n",
    "    target_col=\"label\",\n",
    "    prediction_col=\"classification_proba\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_recs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the train_df by timestamp and get the lastest item features from train_df\n",
    "all_items_df = train_df.sort_values(by=args.timestamp_col, ascending=False)\n",
    "# get the lastest item features from train_df\n",
    "all_items_indices = all_items_df.drop_duplicates(subset=[args.item_col], keep=\"first\")[\"item_indice\"].values\n",
    "all_items_features = item_metadata_pipeline.transform(all_items_df).astype(np.float32)\n",
    "logger.info(\n",
    "    f\"Mean std over categorical and numerical features: {all_items_features.std(axis=0).mean()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = best_model.recommend(\n",
    "    torch.tensor(val_recs_df[\"user_indice\"].values, device=args.device),\n",
    "    torch.tensor(val_recs_df[\"item_sequence\"].values.tolist(), device=args.device),\n",
    "    all_items_indices=torch.tensor(all_items_indices, device=args.device),\n",
    "    all_items_features=torch.tensor(all_items_features, device=args.device),\n",
    "    k=args.top_K,\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_df = pd.DataFrame(recommendations).pipe(\n",
    "    create_rec_df, idm, args.user_col, args.item_col\n",
    ")\n",
    "recommendations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = create_label_df(\n",
    "    val_df,\n",
    "    user_col=args.user_col,\n",
    "    item_col=args.item_col,\n",
    "    rating_col=args.rating_col,\n",
    "    timestamp_col=args.timestamp_col,\n",
    ")\n",
    "label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = merge_recs_with_target(\n",
    "    recommendations_df,\n",
    "    label_df,\n",
    "    k=args.top_K,\n",
    "    user_col=args.user_col,\n",
    "    item_col=args.item_col,\n",
    "    rating_col=args.rating_col,\n",
    ")\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_report = log_ranking_metrics(args, eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = [args]\n",
    "\n",
    "if args.log_to_mlflow:\n",
    "    run_id = trainer.logger.run_id\n",
    "\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        for params in all_params:\n",
    "            params_dict = params.model_dump()\n",
    "            params_ = dict()\n",
    "            for k, v in params_dict.items():\n",
    "                if k == \"top_K\":\n",
    "                    k = \"top_big_K\"\n",
    "                if k == \"top_k\":\n",
    "                    k = \"top_small_k\"\n",
    "                params_[f\"{params.__repr_name__()}.{k}\"] = v\n",
    "            mlflow.log_params(params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hm-scalablerecs-QHnDFvap-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
