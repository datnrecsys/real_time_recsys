{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from loguru import logger\n",
    "from pydantic import BaseModel, model_validator\n",
    "from load_dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from src.utils.split_time_based import train_test_split_timebased\n",
    "from src.utils.embedding_id_mapper import IDMapper\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "_ = load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"run_name\": \"000-data-prep\",\n",
      "  \"run_description\": \"Splitting data into train, val, test sets, then sampling data for quick iteration\",\n",
      "  \"testing\": false,\n",
      "  \"sample_data_persit_path\": \"/home/dinhln/Desktop/real_time_recsys/data_for_ai/interim\",\n",
      "  \"notebook_persit_path\": \"/home/dinhln/Desktop/real_time_recsys/notebooks/data/000-data-prep\",\n",
      "  \"random_seed\": 41,\n",
      "  \"user_col\": \"user_id\",\n",
      "  \"item_col\": \"parent_asin\",\n",
      "  \"rating_col\": \"rating\",\n",
      "  \"timestamp_col\": \"timestamp\",\n",
      "  \"sample_users\": 5000,\n",
      "  \"min_user_interactions\": 5,\n",
      "  \"min_item_interactions\": 10,\n",
      "  \"val_num_days\": 15,\n",
      "  \"test_num_days\": 30,\n",
      "  \"rating_dataset_path\": \"/home/dinhln/Desktop/real_time_recsys/data_for_ai/raw/amz_raw_rating.parquet\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "class Args(BaseModel):\n",
    "    run_name: str = \"000-data-prep\"\n",
    "    run_description: str = \"Splitting data into train, val, test sets, then sampling data for quick iteration\"\n",
    "    testing: bool = False\n",
    "    sample_data_persit_path: str = None    # path of the sampled data: train, test and val\n",
    "    notebook_persit_path: str = None    # path of the notebook\n",
    "    random_seed: int = 41\n",
    "\n",
    "    user_col: str = \"user_id\"\n",
    "    item_col: str = \"parent_asin\"\n",
    "    rating_col: str = \"rating\"\n",
    "    timestamp_col: str = \"timestamp\"\n",
    "\n",
    "    sample_users: int = 5000\n",
    "    min_user_interactions: int = 5\n",
    "    min_item_interactions: int = 10\n",
    "\n",
    "    val_num_days: int = 15\n",
    "    test_num_days: int = 30\n",
    "\n",
    "    rating_dataset_path: str = os.path.abspath(\"../data_for_ai/raw/amz_raw_rating.parquet\")\n",
    "\n",
    "    def init(self):\n",
    "        self.sample_data_persit_path = os.path.abspath(f\"../data_for_ai/interim\")\n",
    "        self.notebook_persit_path = os.path.abspath(f\"./data/{self.run_name}\")\n",
    "        if not self.testing:\n",
    "            os.makedirs(self.sample_data_persit_path, exist_ok=True)\n",
    "            os.makedirs(self.notebook_persit_path, exist_ok=True)\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "args = Args().init()\n",
    "\n",
    "print(args.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from a specific period in order to train the model\n",
    "\n",
    "In notebook 002-simulate-oltp, we can see that the time period from March 2020 to Sep 2020 is the good choice. There are active interactions between users and items in this period and wen can keep the recency. So, we will load data from this period to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (43334103, 4)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all processed chunks into a final DataFrame\n",
    "full_df = pd.read_parquet(args.rating_dataset_path)\n",
    "print(f\"DataFrame shape: {full_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>B01G8JO5F2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2018-04-07 09:23:37.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>B07N69T6TM</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-06-20 18:42:29.731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n",
       "      <td>B083NRGZMM</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-07-18 22:58:37.948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGGZ357AO26RQZVRLGU4D4N52DZQ</td>\n",
       "      <td>B001OC5JKY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2010-11-20 18:41:35.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AG2L7H23R5LLKDKLBEF2Q3L2MVDA</td>\n",
       "      <td>B07CJYMRWM</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-02-17 02:39:41.238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        user_id parent_asin  rating               timestamp\n",
       "0  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  B01G8JO5F2     5.0 2018-04-07 09:23:37.534\n",
       "1  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  B07N69T6TM     1.0 2020-06-20 18:42:29.731\n",
       "2  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  B083NRGZMM     3.0 2022-07-18 22:58:37.948\n",
       "3  AGGZ357AO26RQZVRLGU4D4N52DZQ  B001OC5JKY     5.0 2010-11-20 18:41:35.000\n",
       "4  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  B07CJYMRWM     5.0 2023-02-17 02:39:41.238"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-26 22:56:01.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.split_time_based\u001b[0m:\u001b[36mtrain_test_split_timebased\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mRemoving users from val and test sets...\u001b[0m\n",
      "\u001b[32m2025-03-26 22:56:25.051\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.split_time_based\u001b[0m:\u001b[36mtrain_test_split_timebased\u001b[0m:\u001b[36m37\u001b[0m - \u001b[1mRemoved 25251 users from val set\u001b[0m\n",
      "\u001b[32m2025-03-26 22:56:25.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.split_time_based\u001b[0m:\u001b[36mtrain_test_split_timebased\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mRemoved 43696 users from test set\u001b[0m\n",
      "\u001b[32m2025-03-26 22:56:25.055\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.split_time_based\u001b[0m:\u001b[36mtrain_test_split_timebased\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mTrain set has 18201042 users\u001b[0m\n",
      "\u001b[32m2025-03-26 22:56:25.057\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.split_time_based\u001b[0m:\u001b[36mtrain_test_split_timebased\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mVal set has 8908 users\u001b[0m\n",
      "\u001b[32m2025-03-26 22:56:25.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils.split_time_based\u001b[0m:\u001b[36mtrain_test_split_timebased\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mTest set has 10965 users\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Split train, val, test\n",
    "train_df, val_df, test_df = train_test_split_timebased(\n",
    "    full_df, user_id_col=\"user_id\",\n",
    "        item_id_col=\"parent_asin\",\n",
    "        timestamp_col=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_df[args.timestamp_col].max() < val_df[args.timestamp_col].min(), \"There are overlapping timestamps between train and validation datasets.\"\n",
    "assert val_df[args.timestamp_col].max() < test_df[args.timestamp_col].min(), \"There are overlapping timestamps between validation and test datasets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-26 22:56:36.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m1\u001b[0m - \u001b[1mTrain: (43225743, 4), Val: (10406, 4), Test: (13108, 4)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Train: {train_df.shape}, Val: {val_df.shape}, Test: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling data\n",
    "\n",
    "Just randomly get X users will not guarantee that the output dataset would qualify the condition of **richness**. Instead we take an iterative approach where we gradually drop random users from the dataset while keeping an eye on the conditions and our sampling target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_random_users(df, k=10):\n",
    "    users = df[args.user_col].unique()\n",
    "    np.random.seed(args.random_seed)\n",
    "    to_remove_users = np.random.choice(users, size=k, replace=False)\n",
    "    return df.loc[lambda df: ~df[args.user_col].isin(to_remove_users)]\n",
    "\n",
    "\n",
    "def get_unqualified(df, col: str, threshold: int):\n",
    "    unqualified = df.groupby(col).size().loc[lambda s: s < threshold].index\n",
    "    return unqualified\n",
    "\n",
    "\n",
    "get_unqualified_users = partial(\n",
    "    get_unqualified, col=args.user_col, threshold=args.min_user_interactions\n",
    ")\n",
    "get_unqualified_items = partial(\n",
    "    get_unqualified, col=args.item_col, threshold=args.min_item_interactions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Randomly removing 182010 users - Round 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-26 23:05:17.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mSampling round 1 started\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:04.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mlen(uu)=16192492\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:13.077\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mAfter removing uu: len(sample_df)=17467346\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:21.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mlen(ui)=936952\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:24.617\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mAfter removing ui: len(sample_df)=15134535\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:26.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mSampling round 2 started\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:31.253\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mlen(uu)=328781\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:32.635\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mAfter removing uu: len(sample_df)=13943222\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:37.772\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mlen(ui)=15650\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:39.229\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mAfter removing ui: len(sample_df)=13810437\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:41.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mSampling round 3 started\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:44.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mlen(uu)=22077\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:45.935\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mAfter removing uu: len(sample_df)=13722736\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:50.980\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mlen(ui)=1132\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:52.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mAfter removing ui: len(sample_df)=13712597\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:54.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mSampling round 4 started\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:57.365\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mlen(uu)=1752\u001b[0m\n",
      "\u001b[32m2025-03-26 23:06:58.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mAfter removing uu: len(sample_df)=13705592\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:03.279\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mlen(ui)=78\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:04.722\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mAfter removing ui: len(sample_df)=13704890\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:06.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mSampling round 5 started\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:09.734\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mlen(uu)=125\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:10.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mAfter removing uu: len(sample_df)=13704390\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:15.646\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mlen(ui)=8\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:16.840\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mAfter removing ui: len(sample_df)=13704318\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:18.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mSampling round 6 started\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:21.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mlen(uu)=6\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:22.829\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mAfter removing uu: len(sample_df)=13704294\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:27.900\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mlen(ui)=1\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:28.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m40\u001b[0m - \u001b[1mAfter removing ui: len(sample_df)=13704285\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:30.625\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mSampling round 7 started\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:33.848\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mlen(uu)=1\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:34.569\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m31\u001b[0m - \u001b[1mAfter removing uu: len(sample_df)=13704281\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:39.480\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mlen(ui)=0\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:39.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[1mSampling round 8 started\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:42.731\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[1mlen(uu)=0\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:44.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m36\u001b[0m - \u001b[1mlen(ui)=0\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:46.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m48\u001b[0m - \u001b[1mAfter randomly removing users - round 1: num_users=1473309\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:46.519\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m50\u001b[0m - \u001b[1mNumber of users 1473309 are still greater than expected, keep removing...\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:46.982\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mNumber of val_df records 139 are falling below expected threshold, stop and use `sample_df` as final output...\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:46.983\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mNumber of test_df records 134 are falling below expected threshold, stop and use `sample_df` as final output...\u001b[0m\n",
      "\u001b[32m2025-03-26 23:07:49.103\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mFinal sample sizes: len(sample_users)=1,473,309, len(sample_items)=212,988\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "buffer_perc = 0.2\n",
    "perc_users_removed_each_round = 0.01\n",
    "debug = True\n",
    "keep_random_removing = True\n",
    "r = 1\n",
    "\n",
    "sample_df = train_df.copy()\n",
    "\n",
    "while keep_random_removing:\n",
    "    num_users_removed_each_round = int(\n",
    "        perc_users_removed_each_round * sample_df[args.user_col].nunique()\n",
    "    )\n",
    "    print(\n",
    "        f\"\\n\\nRandomly removing {num_users_removed_each_round} users - Round {r} started\"\n",
    "    )\n",
    "    sample_df = remove_random_users(sample_df, k=num_users_removed_each_round)\n",
    "\n",
    "    keep_removing = True\n",
    "    i = 1\n",
    "\n",
    "    while keep_removing:\n",
    "        if debug:\n",
    "            logger.info(f\"Sampling round {i} started\")\n",
    "        keep_removing = False\n",
    "        uu = get_unqualified_users(sample_df)\n",
    "        if debug:\n",
    "            logger.info(f\"{len(uu)=}\")\n",
    "        if len(uu):\n",
    "            sample_df = sample_df.loc[lambda df: ~df[args.user_col].isin(uu)]\n",
    "            if debug:\n",
    "                logger.info(f\"After removing uu: {len(sample_df)=}\")\n",
    "            assert len(get_unqualified_users(sample_df)) == 0\n",
    "            keep_removing = True\n",
    "        ui = get_unqualified_items(sample_df)\n",
    "        if debug:\n",
    "            logger.info(f\"{len(ui)=}\")\n",
    "        if len(ui):\n",
    "            sample_df = sample_df.loc[lambda df: ~df[args.item_col].isin(ui)]\n",
    "            if debug:\n",
    "                logger.info(f\"After removing ui: {len(sample_df)=}\")\n",
    "            assert len(get_unqualified_items(sample_df)) == 0\n",
    "            keep_removing = True\n",
    "        i += 1\n",
    "\n",
    "    sample_users = sample_df[args.user_col].unique()\n",
    "    sample_items = sample_df[args.item_col].unique()\n",
    "    num_users = len(sample_users)\n",
    "    logger.info(f\"After randomly removing users - round {r}: {num_users=}\")\n",
    "    if num_users > args.sample_users * (1 + buffer_perc):\n",
    "        logger.info(\n",
    "            f\"Number of users {num_users} are still greater than expected, keep removing...\"\n",
    "        )\n",
    "    else:\n",
    "        logger.info(\n",
    "            f\"Number of users {num_users} are falling below expected threshold, stop and use `sample_df` as final output...\"\n",
    "        )\n",
    "        keep_random_removing = False\n",
    "    \n",
    "    val_sample_df = val_df.loc[\n",
    "                lambda df: df[args.user_col].isin(sample_users)\n",
    "                & df[args.item_col].isin(sample_items)\n",
    "            ]\n",
    "    test_sample_df = test_df.loc[\n",
    "                lambda df: df[args.user_col].isin(sample_users)\n",
    "                & df[args.item_col].isin(sample_items)\n",
    "            ]\n",
    "    if (num_val_records := val_sample_df.shape[0]) < 3000:\n",
    "        logger.info(\n",
    "            f\"Number of val_df records {num_val_records:,.0f} are falling below expected threshold, stop and use `sample_df` as final output...\"\n",
    "        )\n",
    "        keep_random_removing = False\n",
    "    if (num_test_records := test_sample_df.shape[0]) < 3000:\n",
    "        logger.info(\n",
    "            f\"Number of test_df records {num_test_records:,.0f} are falling below expected threshold, stop and use `sample_df` as final output...\"\n",
    "        )\n",
    "        keep_random_removing = False\n",
    "\n",
    "    r += 1\n",
    "\n",
    "sample_users = sample_df[args.user_col].unique()\n",
    "sample_items = sample_df[args.item_col].unique()\n",
    "logger.info(f\"Final sample sizes: {len(sample_users)=:,.0f}, {len(sample_items)=:,.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sample_df[args.timestamp_col].max() < val_sample_df[args.timestamp_col].min(), \"There are overlapping timestamps between train and validation datasets.\"\n",
    "assert val_sample_df[args.timestamp_col].max() < test_sample_df[args.timestamp_col].min(), \"There are overlapping timestamps between validation and test datasets.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert val_sample_df.loc[lambda df: ~df[args.user_col].isin(sample_users)].shape[0] == 0, \"Validation DataFrame contains unexpected users.\"\n",
    "assert test_sample_df.loc[lambda df: ~df[args.user_col].isin(sample_users)].shape[0] == 0, \"Test DataFrame contains unexpected users.\"\n",
    "assert val_sample_df.loc[lambda df: ~df[args.item_col].isin(sample_items)].shape[0] == 0, \"Validation DataFrame contains unexpected items.\"\n",
    "assert test_sample_df.loc[lambda df: ~df[args.item_col].isin(sample_items)].shape[0] == 0, \"Test DataFrame contains unexpected items.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(sample_df.groupby(args.user_col).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(sample_df.groupby(args.item_col).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = [\"train\", \"val\", \"test\"]\n",
    "original_length = {\"train\": train_df.shape[0], \"val\": val_df.shape[0], \"test\": test_df.shape[0]}\n",
    "sampled_length = {\"train\": sample_df.shape[0], \"val\": val_sample_df.shape[0], \"test\": test_sample_df.shape[0]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=1, cols=3)\n",
    "\n",
    "# Add data for each subset\n",
    "for i, subset in enumerate(subsets):\n",
    "    row = i // 3 + 1\n",
    "    col = i % 3 +1\n",
    "\n",
    "    # Add trace for 'curr'\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            name=\"original\",\n",
    "            x=[subset],\n",
    "            y=[original_length[subset]],\n",
    "            marker_color = \"lightblue\",\n",
    "            showlegend=(i == 0),\n",
    "            texttemplate=\"%{y:.2}\",\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    # Add trace for 'new'\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            name=\"sample\",\n",
    "            x=[subset],\n",
    "            y=[sampled_length[subset]],\n",
    "            marker_color=\"lightgreen\",\n",
    "            showlegend=(i == 0),\n",
    "            texttemplate=\"%{y:.2}\",\n",
    "        ),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "    # Add diff annotation\n",
    "    difference = (sampled_length[subset] - original_length[subset]) / original_length[\n",
    "        subset\n",
    "    ]\n",
    "    fig.add_annotation(\n",
    "        x=subset,\n",
    "        y=sampled_length[subset] * 1.10,  # Position above the tallest bar\n",
    "        text=f\"Δ={difference:.2%}\",\n",
    "        showarrow=False,\n",
    "        font=dict(color=\"black\", size=14),\n",
    "        row=row,\n",
    "        col=col,\n",
    "    )\n",
    "\n",
    "fig.update_layout(showlegend=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perit the sampled data\n",
    "sample_df.to_parquet(f\"{args.sample_data_persit_path}/train_sample_interactions_{args.sample_users}u.parquet\")\n",
    "val_sample_df.to_parquet(f\"{args.sample_data_persit_path}/val_sample_interactions_{args.sample_users}u.parquet\")\n",
    "test_sample_df.to_parquet(f\"{args.sample_data_persit_path}/test_sample_interactions_{args.sample_users}u.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to version your data with dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_df = pd.read_parquet(f\"{args.sample_data_persit_path}/train_sample_interactions_{args.sample_users}u.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interactions_over_time(df):\n",
    "    df = df.assign(timestamp=df[args.timestamp_col].dt.date)\n",
    "    plot_df = df.groupby(args.timestamp_col).size()\n",
    "\n",
    "    fig = px.line(\n",
    "        x=plot_df.index,\n",
    "        y=plot_df.values,\n",
    "        labels={\"x\": \"Date\", \"y\": \"Number of Interactions\"},\n",
    "        title=\"Interactions Over Time\",\n",
    "        height=500,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(yaxis=dict(showticklabels=True, tickformat=\",\"))\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user_id</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>2020-11-21 11:31:14.232</td>\n",
       "      <td>AH6U3RG4SKWXF4KNH3RC6VD5P4QQ</td>\n",
       "      <td>B0953YFR2M</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>2021-05-25 12:17:57.423</td>\n",
       "      <td>AGBOFSSHGILKH73MJZUUOTRCD4CA</td>\n",
       "      <td>B0BW9DSR52</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3303</th>\n",
       "      <td>2020-07-19 13:47:10.212</td>\n",
       "      <td>AEEDFUQ7SXVEZ4VBHTED5D6MZ7QA</td>\n",
       "      <td>B08YX4HGRY</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>2020-10-19 19:13:42.773</td>\n",
       "      <td>AHR6RGZTLOMBD7EBF3OK43JWMGNQ</td>\n",
       "      <td>B09NTXBJDM</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3547</th>\n",
       "      <td>2021-05-17 23:23:05.132</td>\n",
       "      <td>AGZMKHWSCB3UXDGFUPFRZSL4EAWQ</td>\n",
       "      <td>B08XD3WW2H</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24565423</th>\n",
       "      <td>2020-07-13 10:51:57.066</td>\n",
       "      <td>AFGEK77LF27ECZWRR5J2TZGEOJ7A</td>\n",
       "      <td>B00KDSGIPK</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24565424</th>\n",
       "      <td>2020-07-13 10:55:37.739</td>\n",
       "      <td>AFGEK77LF27ECZWRR5J2TZGEOJ7A</td>\n",
       "      <td>B017250D16</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24565425</th>\n",
       "      <td>2020-07-13 11:08:07.474</td>\n",
       "      <td>AFGEK77LF27ECZWRR5J2TZGEOJ7A</td>\n",
       "      <td>B076TCPKJT</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24565426</th>\n",
       "      <td>2020-09-02 18:58:47.052</td>\n",
       "      <td>AFGEK77LF27ECZWRR5J2TZGEOJ7A</td>\n",
       "      <td>B01MDKA8EH</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24565428</th>\n",
       "      <td>2020-12-02 01:05:35.494</td>\n",
       "      <td>AFGEK77LF27ECZWRR5J2TZGEOJ7A</td>\n",
       "      <td>B07DD3Z1VV</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183267 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       timestamp                       user_id parent_asin  \\\n",
       "2931     2020-11-21 11:31:14.232  AH6U3RG4SKWXF4KNH3RC6VD5P4QQ  B0953YFR2M   \n",
       "2992     2021-05-25 12:17:57.423  AGBOFSSHGILKH73MJZUUOTRCD4CA  B0BW9DSR52   \n",
       "3303     2020-07-19 13:47:10.212  AEEDFUQ7SXVEZ4VBHTED5D6MZ7QA  B08YX4HGRY   \n",
       "3375     2020-10-19 19:13:42.773  AHR6RGZTLOMBD7EBF3OK43JWMGNQ  B09NTXBJDM   \n",
       "3547     2021-05-17 23:23:05.132  AGZMKHWSCB3UXDGFUPFRZSL4EAWQ  B08XD3WW2H   \n",
       "...                          ...                           ...         ...   \n",
       "24565423 2020-07-13 10:51:57.066  AFGEK77LF27ECZWRR5J2TZGEOJ7A  B00KDSGIPK   \n",
       "24565424 2020-07-13 10:55:37.739  AFGEK77LF27ECZWRR5J2TZGEOJ7A  B017250D16   \n",
       "24565425 2020-07-13 11:08:07.474  AFGEK77LF27ECZWRR5J2TZGEOJ7A  B076TCPKJT   \n",
       "24565426 2020-09-02 18:58:47.052  AFGEK77LF27ECZWRR5J2TZGEOJ7A  B01MDKA8EH   \n",
       "24565428 2020-12-02 01:05:35.494  AFGEK77LF27ECZWRR5J2TZGEOJ7A  B07DD3Z1VV   \n",
       "\n",
       "          rating  \n",
       "2931         5.0  \n",
       "2992         5.0  \n",
       "3303         5.0  \n",
       "3375         4.0  \n",
       "3547         4.0  \n",
       "...          ...  \n",
       "24565423     5.0  \n",
       "24565424     4.0  \n",
       "24565425     3.0  \n",
       "24565426     4.0  \n",
       "24565428     5.0  \n",
       "\n",
       "[183267 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up idm\n",
    "# Sorted to make sure that even rerun we get same idm mapping\n",
    "unique_user_ids = sorted(train_sample_df[args.user_col].unique())\n",
    "unique_item_ids = sorted(train_sample_df[args.item_col].unique())\n",
    "logger.info(f\"Number of unique users: {len(unique_user_ids):,.0f}\")\n",
    "logger.info(f\"Number of unique items: {len(unique_item_ids):,.0f}\")\n",
    "idm = IDMapper()\n",
    "idm.fit(unique_user_ids, unique_item_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idm.save(f\"{args.notebook_persit_path}/idm_{args.sample_users}u.json\")\n",
    "idm_persist_fp = f\"{args.notebook_persit_path}/idm_{args.sample_users}u.json\"\n",
    "idm = IDMapper().load(idm_persist_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idm.item_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, _ in idm.item_to_index.items():\n",
    "    assert type(k) is str, \"Type of user id should be string\"\n",
    "for k,_ in idm.user_to_index.items():\n",
    "    assert type(k) is str, \"Type of item id should be string\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
