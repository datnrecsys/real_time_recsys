{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydantic import BaseModel\n",
    "import sys\n",
    "import os\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from loguru import logger\n",
    "from load_dotenv import load_dotenv\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "from src.utils.embedding_id_mapper import IDMapper\n",
    "from src.algo.mf.model import MatrixFactorizationRating\n",
    "from src.algo.mf.dataset import UserItemRatingDFDataset\n",
    "from src.algo.mf.trainer import MFLitModule\n",
    "from src.eval.utils import create_rec_df, create_label_df, merge_recs_with_target\n",
    "from src.eval.log_metrics import log_ranking_metrics, log_classification_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(BaseModel):\n",
    "    testing: bool = False\n",
    "    log_to_mlflow: bool = True\n",
    "    experiment_name: str = \"first-attempt\"\n",
    "    run_name: str = \"004-mf-cf\"\n",
    "    notebook_persit_dp: str = None\n",
    "    \n",
    "    user_col: str = \"user_id\"\n",
    "    item_col: str = \"parent_asin\"\n",
    "    rating_col: str = \"rating\"\n",
    "    timestamp_col: str = \"timestamp\"\n",
    "    group_name: str = \"mf-cf\"\n",
    "\n",
    "    top_K: int = 100\n",
    "    top_k: int = 10\n",
    "\n",
    "    batch_size: int = 64\n",
    "    embedding_dim: int = 128\n",
    "    learning_rate: float = 0.001\n",
    "    l2_reg: float = 1e-4\n",
    "    early_stopping_patience: int = 10\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    max_epochs: int = 100\n",
    "\n",
    "    train_data_fp: str = os.path.abspath(\"../data_for_ai/interim/train_sample_interactions_16407u_neg.parquet\")\n",
    "    val_data_fp: str = os.path.abspath(\"../data_for_ai/interim/val_sample_interactions_16407u_neg.parquet\")\n",
    "\n",
    "    def init(self):\n",
    "        self.notebook_persit_dp = os.path.abspath(f\"data/{self.experiment_name}/{self.run_name}\")\n",
    "\n",
    "        if not (mlflow_uri := os.environ.get(\"MLFLOW_TRACKING_URI\")):\n",
    "            self.log_to_mlflow = False\n",
    "            logger.warning(\"MLFlow is not enabled. Turn off tracking to Mlflow.\")\n",
    "\n",
    "        if self.log_to_mlflow:\n",
    "            logger.info(\n",
    "                f\"Setting up Mlflow experiment: {self.experiment_name}, run_name: {self.run_name}\"\n",
    "            )\n",
    "\n",
    "            self._mlf_logger = MLFlowLogger(\n",
    "                experiment_name=self.experiment_name,\n",
    "                run_name=self.run_name,\n",
    "                tracking_uri=mlflow_uri,\n",
    "                log_model=True,\n",
    "            )\n",
    "\n",
    "        if not self.testing:\n",
    "            os.makedirs(self.notebook_persit_dp, exist_ok=True)\n",
    "        return self\n",
    "    \n",
    "args = Args().init()\n",
    "print(args.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_parquet(args.train_data_fp)\n",
    "val_df = pd.read_parquet(args.val_data_fp)\n",
    "\n",
    "assert set(val_df[args.user_col].unique()).issubset(set(train_df[args.user_col].unique())), \"Validation users must be present in training users.\"\n",
    "\n",
    "assert set(val_df[args.item_col].unique()).issubset(set(train_df[args.item_col].unique())), \"Validation items must be present in training items.\"\n",
    "assert train_df[args.timestamp_col].max() < val_df[args.timestamp_col].min(), \"Validation data must be after training data. Otherwise, its a data contamination problem.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert user_id and item_id into indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idm_path = os.path.abspath(\"../data_for_ai/interim/idm_16407u.json\")\n",
    "idm = IDMapper().load(idm_path)\n",
    "idm.get_user_id(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.pipe(idm.map_indices)\n",
    "val_df = val_df.pipe(idm.map_indices)\n",
    "\n",
    "assert idm.unknown_item_index not in train_df[\"item_indice\"].values, \"Unknown item index must be present in training data.\"\n",
    "assert idm.unknown_user_index not in train_df[\"user_indice\"].values, \"Unknown user index must be present in training data.\"\n",
    "assert idm.unknown_item_index not in val_df[\"item_indice\"].values, \"Unknown item index must be present in validation data.\"\n",
    "assert idm.unknown_user_index not in val_df[\"user_indice\"].values, \"Unknown user index must be present in validation data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_df.groupby(args.user_col)[args.item_col].nunique().min() >= 5, \"Each user must have at least five items.\"\n",
    "assert train_df.groupby(args.item_col)[args.user_col].nunique().min() >= 10, \"Each item must have at least ten users.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(n_user, n_items, embedding_dim):\n",
    "    \"\"\"\n",
    "    Initialize the model with the given parameters.\n",
    "    \"\"\"\n",
    "    model = MatrixFactorizationRating(\n",
    "        num_users = n_user,\n",
    "        num_items = n_items,\n",
    "        embedding_dim = embedding_dim,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 8\n",
    "batch_size = 2\n",
    "\n",
    "# Mock data\n",
    "user_indices = [0, 0, 1, 2, 2]\n",
    "item_indices = [0, 1, 2, 3, 4]\n",
    "timestamps = [0, 1, 2, 3, 4]\n",
    "ratings = [0, 4, 5, 3, 0]\n",
    "\n",
    "n_users = len(set(user_indices))\n",
    "n_items = len(set(item_indices))\n",
    "\n",
    "train_test_df = pd.DataFrame(\n",
    "    {\n",
    "        \"user_indice\": user_indices,\n",
    "        \"item_indice\": item_indices,\n",
    "        args.timestamp_col: timestamps,\n",
    "        args.rating_col: ratings,\n",
    "    }\n",
    ")\n",
    "\n",
    "model = init_model(n_users, n_items, embedding_dim)\n",
    "\n",
    "# Example forward pass\n",
    "model.eval()\n",
    "user = torch.tensor([0])\n",
    "target_item = torch.tensor([2])\n",
    "predictions = model.predict(user, target_item)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UserItemRatingDFDataset(\n",
    "    df = train_test_df,\n",
    "    user_col = \"user_indice\",\n",
    "    item_col = \"item_indice\",\n",
    "    rating_col = args.rating_col,\n",
    "    timestamp_col = args.timestamp_col,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_input in train_loader:\n",
    "    print(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "lit_model = MFLitModule(model, log_dir=args.notebook_persit_dp, top_K = 3,top_k=3, idm = idm)\n",
    "# train model\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=f\"{args.notebook_persit_dp}/test\",\n",
    "    max_epochs=150,\n",
    "    accelerator=\"auto\",\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "trainer.fit(\n",
    "    model=lit_model, train_dataloaders=train_loader, val_dataloaders=train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_dataset = UserItemRatingDFDataset(\n",
    "    train_df, \"user_indice\", \"item_indice\", args.rating_col, args.timestamp_col\n",
    ")\n",
    "val_rating_dataset = UserItemRatingDFDataset(\n",
    "    val_df, \"user_indice\", \"item_indice\", args.rating_col, args.timestamp_col\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    rating_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_rating_dataset, batch_size=args.batch_size, shuffle=False, drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_indices = train_df[args.item_col].unique()\n",
    "user_indices = train_df[args.user_col].unique()\n",
    "n_items = len(item_indices)\n",
    "n_users = len(user_indices)\n",
    "\n",
    "logger.info(f\"Number of users: {n_users}, Number of items: {n_items}\")\n",
    "model = init_model(n_users, n_items, args.embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfir 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=10, mode=\"min\", verbose=False\n",
    ")\n",
    "\n",
    "model = init_model(n_users, n_items, args.embedding_dim)\n",
    "lit_model = MFLitModule(\n",
    "    model,\n",
    "    learning_rate=args.learning_rate,\n",
    "    l2_reg=args.l2_reg,\n",
    "    log_dir=args.notebook_persit_dp,\n",
    "    accelerator=args.device,\n",
    "    idm= idm\n",
    ")\n",
    "\n",
    "log_dir = f\"{args.notebook_persit_dp}/logs/overfit\"\n",
    "\n",
    "# train model\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=log_dir,\n",
    "    accelerator=args.device if args.device else \"auto\",\n",
    "    max_epochs=300,\n",
    "    overfit_batches=1,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "trainer.fit(\n",
    "    model=lit_model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=train_loader,\n",
    ")\n",
    "logger.info(f\"Logs available at {trainer.log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=10, mode=\"min\", verbose=False, min_delta=0.02\n",
    ")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=f\"{args.notebook_persit_dp}/checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "model = init_model(n_users, n_items, args.embedding_dim)\n",
    "lit_model = MFLitModule(\n",
    "    model,\n",
    "    learning_rate=args.learning_rate,\n",
    "    l2_reg=args.l2_reg,\n",
    "    log_dir=args.notebook_persit_dp,\n",
    "    accelerator=args.device,\n",
    "    idm= idm\n",
    ")\n",
    "\n",
    "log_dir = f\"{args.notebook_persit_dp}/logs/run\"\n",
    "\n",
    "# train model\n",
    "trainer = L.Trainer(\n",
    "    default_root_dir=log_dir,\n",
    "    accelerator=args.device if args.device else \"auto\",\n",
    "    max_epochs=args.max_epochs,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    logger=args._mlf_logger if args.log_to_mlflow else None,\n",
    ")\n",
    "trainer.fit(\n",
    "    model=lit_model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")\n",
    "\n",
    "#https://github.com/Lightning-AI/pytorch-lightning/pull/20669/commits/429f732a0528c558e701da7ec01e51c1e2e4f32e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
